{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.6.0\n      /_/\n\nUsing Python version 2.7.13 (default, Dec 20 2016 23:09:15)\nSparkContext available as sc, HiveContext available as sqlContext.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/lib/spark/\"\n",
    "os.environ[\"PYSPARK_PYTHON\"]= \"/home/cloudera/anaconda2/bin/python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]= \"/home/cloudera/anaconda2/bin/python\"\n",
    "os.environ[\"SPARK_YARN_USER_ENV\"]= \"/home/cloudera/anaconda2/bin/python\"\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.8.1-src.zip'))\n",
    "execfile(os.path.join(spark_home, 'python/pyspark/shell.py'))\n",
    "\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Kaggle Instatcart Market Basket analysis has data with more than 2 million rows in orders_product_prior and other datat set also has around a milloin rows that requires manipulation using partition and parallel processing.\n",
    "##### For this reason In am using spark in jupyter and building my complete project in pycharm IDE. \n",
    "##### Created a virtual enviroment with spark libraries and anaconda python 2.7, initial settings are shown inthe first cell. \n",
    "##### Changed the 'dfs.blocksize' paramemter in the hdfs-default.xml to 64X1024X1024 bytes. \n",
    "##### This will increase parallisum in the hdfs processing for aggregating dataset in memory becomes faster.\n",
    "##### for more information check this blog http://www.bigsynapse.com/spark-input-output.\n",
    "##### Though I am using Pseudorandom distribution on cloudera quick start VM 5.8 it might not have any performance benifit as expected as machine is single but one cn try increasing the no of core and the executor memory.\n",
    "##### default driver memory given is 5g in my VM , since in local mode executor is inside the driver so can't get memory more than that.##### underline properties to configure is spark  --num-executors <is one as installation is standalone> --executor-cores <depends on core of processor> --executor-memory <Memory available to the VM>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Order Schema \n",
    "##### order_id, user_id, eval_set, order_number, order_dow, order_hour_of_day, days_since_prior_orde\n",
    "##### read data with partition of three since data size of orders file is more than 110 mb we use 4 partition of size 3*32 mb\n",
    "##### 32 mb is the local file system block size in whicj data is stored\n",
    "##### 128 mb block size is the size of memory the name node referrences more 32 size blockes can be added to it.\n",
    "##### It is for the namenode to decide in which 128 mb block refrence the 32 mb block can go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orders = sc.textFile(\"file:///home/cloudera/Desktop/kaggle/orders.csv\", 4).filter(lambda x: 'order_id' not in x).map(lambda x : x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'2539329', u'1', u'prior', u'1', u'2', u'08', u'']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Order_Product_Schema\n",
    "##### order_id, Product_id, \n",
    "##### Here for we are taking the partition of 'order_products__prior' to be 18 as it is of more 550 mb \n",
    "##### similarly for 'order_products__train'  it is  default 0 partiton as the size is less than block size 32 mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orders_prior = sc.textFile(\"file:///home/cloudera/Desktop/kaggle/order_products__prior.csv\", 18).filter(lambda x: 'order_id' not in x).\\\n",
    "    map(lambda x : x.split(','))\n",
    "\n",
    "orders_train = sc.textFile(\"file:///home/cloudera/Desktop/kaggle/order_products__train.csv\").filter(lambda x: 'order_id' not in x).\\\n",
    "    map(lambda x : x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'2', u'33120', u'1', u'1']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_prior.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save all rdds created to hadoop file system in distributed fashion based on the no of the partions inthe rdd while loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.map(lambda x : ','.join(x)).saveAsTextFile(\"/user/cloudera/kaggle/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_prior.map(lambda x : ','.join(x)).saveAsTextFile(\"/user/cloudera/kaggle/orders_prior\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_train.map(lambda x : ','.join(x)).saveAsTextFile(\"/user/cloudera/kaggle/orders_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define null prser to parse the blank strings \n",
    "def nullparse(x):\n",
    "    if x =='':\n",
    "        x=0\n",
    "    else:\n",
    "        float(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= sc.textFile(\"/user/cloudera/kaggle/orders\").map(lambda x: x.split(',')).map(lambda x : (int(x[0]),(x[1],x[2],x[3],x[4],x[5],str(nullparse(x[6])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=sc.textFile(\"/user/cloudera/kaggle/orders_prior\").map(lambda x: x.split(',')).map(lambda x : (int(x[0]), x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z= sc.textFile(\"/user/cloudera/kaggle/orders_train\").map(lambda x: x.split(',')).map(lambda x : (int(x[0]), x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.filter(lambda x : x[1][1]!=\"test\").mapValues(lambda x : x[5]).reduceByKey(lambda x,y : x+y).\\\n",
    "    sortBy(lambda x : -x[1]).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine all products purchased for a perticular order id in one string sperated by space for order_product_prior data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3145734, u'45223 32740 46616 8230 6975 47668 3582'),\n (2621448, u'37718 38544 2748 47209 45504 28699 8006 41665 2078 35887'),\n (2097162, u'48988 37774 24852 19057')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.groupByKey().mapValues(lambda x : ' '.join(x)).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the intermediate result to the HDFS to reduce the time for further agregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.groupByKey().mapValues(lambda x : ' '.join(x)).map(lambda x : (str(x[0]),x[1])).map(lambda x : ','.join(x)).saveAsTextFile(\"/user/cloudera/kaggle/order_prior_inter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "order_prod_prio_inter = sc.textFile(\"/user/cloudera/kaggle/order_prior_inter\").map(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'3145734', u'45223 32740 46616 8230 6975 47668 3582'],\n [u'2621448', u'37718 38544 2748 47209 45504 28699 8006 41665 2078 35887']]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_prod_prio_inter.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "order_prod_prio_inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2539329, (u'1', u'prior', u'1', u'2', u'08', '0')),\n (2398795, (u'1', u'prior', u'2', u'3', u'07', '15.0'))]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}